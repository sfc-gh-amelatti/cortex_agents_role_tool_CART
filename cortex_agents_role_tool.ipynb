{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Automated Least-Permissions Role Generation for Snowflake Cortex Agents\n",
        "\n",
        "This notebook automates the process of generating least-privilege SQL scripts for Snowflake Cortex Agents by:\n",
        "\n",
        "1. **REST API Call**: Making REST API calls to DESCRIBE agent objects and extracting relevant analyst + search tools + agent database.schema information\n",
        "2. **SQL Query Generation**: From the curl output, generating SQL queries for individual semantic views from the agent object to identify tables that need permissions\n",
        "3. **Permission Script Output**: Returning comprehensive SQL output for permissions that an admin can execute\n",
        "\n",
        "## Prerequisites\n",
        "- Snowflake connection configured\n",
        "- Appropriate permissions to query agent objects and semantic views\n",
        "- Python packages: requests, pyyaml, snowflake-snowpark-python\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import json\n",
        "import yaml\n",
        "import requests\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Set, Tuple, Optional\n",
        "import os\n",
        "import tempfile\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Snowflake imports\n",
        "import snowflake.connector\n",
        "from snowflake.snowpark.session import Session\n",
        "from snowflake.snowpark.functions import col, lit\n",
        "from snowflake.snowpark.types import StringType\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: REST API Call to DESCRIBE Agent Object\n",
        "\n",
        "This section handles making REST API calls to Snowflake to retrieve agent specifications and extract relevant information about analyst tools, search tools, and agent database/schema details.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_agent_describe_api_call(account_url: str, agent_database: str, agent_schema: str, agent_name: str, bearer_token: str) -> Optional[Dict]:\n",
        "    \"\"\"\n",
        "    Make REST API call to DESCRIBE agent object in Snowflake.\n",
        "    \n",
        "    Args:\n",
        "        account_url: Snowflake account URL (e.g., 'https://orgname-accountname.snowflakecomputing.com')\n",
        "        agent_database: Database containing the agent\n",
        "        agent_schema: Schema containing the agent\n",
        "        agent_name: Name of the agent\n",
        "        bearer_token: Bearer token for authentication\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary containing agent specification or None if failed\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Construct the API endpoint URL\n",
        "        api_url = f\"{account_url}/api/v2/databases/{agent_database.lower()}/schemas/{agent_schema.lower()}/agents/{agent_name}\"\n",
        "        \n",
        "        # Set up headers\n",
        "        headers = {\n",
        "            'Content-Type': 'application/json',\n",
        "            'Accept': 'application/json',\n",
        "            'Authorization': f'Bearer {bearer_token}'\n",
        "        }\n",
        "        \n",
        "        # Make the API call\n",
        "        response = requests.get(api_url, headers=headers)\n",
        "        \n",
        "        if response.status_code == 200:\n",
        "            return response.json()\n",
        "        else:\n",
        "            print(f\"API call failed with status code: {response.status_code}\")\n",
        "            print(f\"Response: {response.text}\")\n",
        "            return None\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"Error making API call: {e}\")\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Using the REST API Function (Real Implementation)\n",
        "\n",
        "To use the `make_agent_describe_api_call()` function with actual Snowflake REST API calls, you'll need:\n",
        "\n",
        "1. **Snowflake Account URL**: Your Snowflake account URL\n",
        "2. **Bearer Token**: A valid authentication token\n",
        "3. **Agent Details**: Database, schema, and agent name\n",
        "\n",
        "Here's how to use it:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Using the REST API function with real Snowflake API calls\n",
        "# Uncomment and modify the following code to make actual API calls\n",
        "\n",
        "def make_real_api_call_example(SNOWFLAKE_ACCOUNT_URL, AGENT_DATABASE, AGENT_SCHEMA, AGENT_NAME):\n",
        "    \"\"\"\n",
        "    Example function showing how to make a real API call.\n",
        "    Modify the parameters below for your environment.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Configuration - MODIFY THESE VALUES FOR YOUR ENVIRONMENT\n",
        "    # SNOWFLAKE_ACCOUNT_URL = \"https://orgname-accountname.snowflakecomputing.com\"\n",
        "    # AGENT_DATABASE = \"yourDB\"\n",
        "    # AGENT_SCHEMA = \"yourSchema\"\n",
        "    # AGENT_NAME = \"agentName\"\n",
        "    \n",
        "    bearer_token = os.getenv(\"SNOWFLAKE_BEARER_TOKEN\")\n",
        "    \n",
        "    if not bearer_token:\n",
        "        print(\"No bearer token available. Please set SNOWFLAKE_BEARER_TOKEN environment variable\")\n",
        "        print(\"or ensure you have an active Snowflake session.\")\n",
        "        return None\n",
        "    \n",
        "    # Make the API call\n",
        "    print(f\"Making API call for agent: {AGENT_DATABASE}.{AGENT_SCHEMA}.{AGENT_NAME}\")\n",
        "    agent_data = make_agent_describe_api_call(\n",
        "        account_url=SNOWFLAKE_ACCOUNT_URL,\n",
        "        agent_database=AGENT_DATABASE,\n",
        "        agent_schema=AGENT_SCHEMA,\n",
        "        agent_name=AGENT_NAME,\n",
        "        bearer_token=bearer_token\n",
        "    )\n",
        "    \n",
        "    if agent_data:\n",
        "        print(\"✅ API call successful!\")\n",
        "        print(f\"Agent Name: {agent_data.get('name')}\")\n",
        "        print(f\"Database: {agent_data.get('database_name')}\")\n",
        "        print(f\"Schema: {agent_data.get('schema_name')}\")\n",
        "        return agent_data\n",
        "    else:\n",
        "        print(\"❌ API call failed\")\n",
        "        return None\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Modify the inputs to the function call to point at the agent object you want to generate permissions for\n",
        "\n",
        "example_agent_data = make_real_api_call_example(\n",
        "    \"https://orgname-accountname.snowflakecomputing.com\",\n",
        "    \"yourDB\",\n",
        "    \"yourSchema\",\n",
        "    \"agentName\")\n",
        "example_agent_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Parse Agent Specification and Extract Tool Information\n",
        "\n",
        "This section parses the agent specification JSON to extract information about:\n",
        "- Cortex Analyst (Text-to-SQL) tools and their semantic views\n",
        "- Cortex Search tools and their search services\n",
        "- Database and schema usage requirements\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def parse_agent_tools(agent_data: Dict) -> Dict:\n",
        "    \"\"\"\n",
        "    Parse agent specification to extract tool information.\n",
        "    \n",
        "    Args:\n",
        "        agent_data: Agent specification dictionary from REST API\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary containing parsed tool information\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Parse the nested agent_spec JSON string\n",
        "        agent_spec = json.loads(agent_data[\"agent_spec\"])\n",
        "        tools = agent_spec.get(\"tools\", [])\n",
        "        tool_resources = agent_spec.get(\"tool_resources\", {})\n",
        "        \n",
        "        # Initialize collections\n",
        "        semantic_views = set()\n",
        "        semantic_model_files = set()  # Track semantic model files\n",
        "        search_services = set()\n",
        "        procedures = set()  # Track procedures\n",
        "        databases = set()\n",
        "        schemas = set()\n",
        "        tool_details = []\n",
        "        tool_warehouses = {}  # Track warehouses for specific tools\n",
        "        \n",
        "        # Process each tool\n",
        "        for tool in tools:\n",
        "            tool_spec = tool.get(\"tool_spec\", {})\n",
        "            tool_type = tool_spec.get(\"type\")\n",
        "            tool_name = tool_spec.get(\"name\")\n",
        "            tool_description = tool_spec.get(\"description\", \"\")\n",
        "            \n",
        "            if not tool_name or tool_name not in tool_resources:\n",
        "                continue\n",
        "                \n",
        "            resource_details = tool_resources[tool_name]\n",
        "            \n",
        "            tool_info = {\n",
        "                \"name\": tool_name,\n",
        "                \"type\": tool_type,\n",
        "                \"description\": tool_description,\n",
        "                \"resources\": resource_details\n",
        "            }\n",
        "            \n",
        "            # Extract warehouse information from execution_environment\n",
        "            execution_env = resource_details.get(\"execution_environment\", {})\n",
        "            if execution_env and execution_env.get(\"type\") == \"warehouse\":\n",
        "                warehouse_name = execution_env.get(\"warehouse\", \"\")\n",
        "                if warehouse_name:  # Only track non-empty warehouses\n",
        "                    tool_warehouses[tool_name] = warehouse_name\n",
        "                    tool_info[\"warehouse\"] = warehouse_name\n",
        "            \n",
        "            if tool_type == \"cortex_analyst_text_to_sql\":\n",
        "                semantic_view = resource_details.get(\"semantic_view\")\n",
        "                semantic_model_file = resource_details.get(\"semantic_model_file\")\n",
        "                \n",
        "                if semantic_view:\n",
        "                    semantic_views.add(semantic_view)\n",
        "                    tool_info[\"semantic_view\"] = semantic_view\n",
        "                    \n",
        "                    # Extract database and schema from semantic view\n",
        "                    parts = semantic_view.split('.')\n",
        "                    if len(parts) == 3:\n",
        "                        databases.add(parts[0])\n",
        "                        schemas.add(f\"{parts[0]}.{parts[1]}\")\n",
        "                \n",
        "                if semantic_model_file:\n",
        "                    semantic_model_files.add(semantic_model_file)\n",
        "                    tool_info[\"semantic_model_file\"] = semantic_model_file\n",
        "                    \n",
        "                    # Extract stage path from semantic model file\n",
        "                    # Format: @DATABASE.SCHEMA.STAGE_NAME/file.yaml\n",
        "                    if semantic_model_file.startswith('@'):\n",
        "                        stage_path = semantic_model_file[1:]  # Remove @ prefix\n",
        "                        path_parts = stage_path.split('/')\n",
        "                        if len(path_parts) >= 1:\n",
        "                            stage_identifier = path_parts[0]  # DATABASE.SCHEMA.STAGE_NAME\n",
        "                            stage_parts = stage_identifier.split('.')\n",
        "                            if len(stage_parts) >= 3:\n",
        "                                databases.add(stage_parts[0])\n",
        "                                schemas.add(f\"{stage_parts[0]}.{stage_parts[1]}\")\n",
        "                                tool_info[\"stage\"] = stage_identifier\n",
        "                        \n",
        "            elif tool_type == \"cortex_search\":\n",
        "                # search_service = resource_details.get(\"name\")\n",
        "                search_service = resource_details.get(\"search_service\") or resource_details.get(\"name\")\n",
        "                if search_service:\n",
        "                    search_services.add(search_service)\n",
        "                    tool_info[\"search_service\"] = search_service\n",
        "                    \n",
        "                    # Extract database and schema from search service\n",
        "                    parts = search_service.split('.')\n",
        "                    if len(parts) == 3:\n",
        "                        databases.add(parts[0])\n",
        "                        schemas.add(f\"{parts[0]}.{parts[1]}\")\n",
        "            \n",
        "            elif tool_type == \"generic\":\n",
        "                # Handle generic tools that might be procedures\n",
        "                procedure_type = resource_details.get(\"type\")\n",
        "                if procedure_type == \"procedure\":\n",
        "                    procedure_identifier = resource_details.get(\"identifier\")\n",
        "                    procedure_name = resource_details.get(\"name\")\n",
        "                    \n",
        "                    if procedure_identifier and procedure_name:\n",
        "                        # Combine identifier and name to get full procedure signature\n",
        "                        # identifier: \"db.schema.procedure\"\n",
        "                        # name: \"procedure(VARCHAR, VARCHAR, VARCHAR)\"\n",
        "                        # result: \"db.schema.procedure(VARCHAR, VARCHAR, VARCHAR)\"\n",
        "                        full_procedure_signature = f\"{procedure_identifier}({procedure_name.split('(')[1]}\"\n",
        "                        procedures.add(full_procedure_signature)\n",
        "                        tool_info[\"procedure\"] = full_procedure_signature\n",
        "                        \n",
        "                        # Extract database and schema from procedure identifier\n",
        "                        # Format: DATABASE.SCHEMA.PROCEDURE_NAME\n",
        "                        parts = procedure_identifier.split('.')\n",
        "                        if len(parts) >= 3:\n",
        "                            databases.add(parts[0])\n",
        "                            schemas.add(f\"{parts[0]}.{parts[1]}\")\n",
        "                        print(f\"Tool {tool_name} requires procedure: {full_procedure_signature}\")\n",
        "            \n",
        "            tool_details.append(tool_info)\n",
        "        \n",
        "        return {\n",
        "            \"semantic_views\": list(semantic_views),\n",
        "            \"semantic_model_files\": list(semantic_model_files),  # Include semantic model files\n",
        "            \"search_services\": list(search_services),\n",
        "            \"procedures\": list(procedures),  # Include procedures\n",
        "            \"databases\": list(databases),\n",
        "            \"schemas\": list(schemas),\n",
        "            \"tool_details\": tool_details,\n",
        "            \"tool_warehouses\": tool_warehouses,  # Include warehouse mapping\n",
        "            \"agent_name\": agent_data.get(\"name\"),\n",
        "            \"agent_database\": agent_data.get(\"database_name\"),\n",
        "            \"agent_schema\": agent_data.get(\"schema_name\")\n",
        "        }\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing agent tools: {e}\")\n",
        "        return {\n",
        "            \"semantic_views\": [],\n",
        "            \"semantic_model_files\": [],  # Include semantic model files\n",
        "            \"search_services\": [],\n",
        "            \"procedures\": [],\n",
        "            \"databases\": [],\n",
        "            \"schemas\": [],\n",
        "            \"tool_details\": [],\n",
        "            \"tool_warehouses\": {},\n",
        "            \"agent_name\": \"UNKNOWN\",\n",
        "            \"agent_database\": \"UNKNOWN\",\n",
        "            \"agent_schema\": \"UNKNOWN\"\n",
        "        }\n",
        "\n",
        "print(\"Agent parsing functions defined successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Parse the example agent data\n",
        "if example_agent_data:\n",
        "    parsed_tools = parse_agent_tools(example_agent_data)\n",
        "    \n",
        "    print(\"Parsed Tool Information:\")\n",
        "    print(f\"Agent: {parsed_tools['agent_name']}\")\n",
        "    print(f\"Agent Location: {parsed_tools['agent_database']}.{parsed_tools['agent_schema']}\")\n",
        "    print(f\"\\nSemantic Views: {parsed_tools['semantic_views']}\")\n",
        "    print(f\"Search Services: {parsed_tools['search_services']}\")\n",
        "    print(f\"Databases: {parsed_tools['databases']}\")\n",
        "    print(f\"Schemas: {parsed_tools['schemas']}\")\n",
        "    \n",
        "    print(\"\\nDetailed Tool Information:\")\n",
        "    for tool in parsed_tools['tool_details']:\n",
        "        print(f\"\\nTool: {tool['name']}\")\n",
        "        print(f\"  Type: {tool['type']}\")\n",
        "        print(f\"  Description: {tool['description'][:100]}...\" if len(tool['description']) > 100 else f\"  Description: {tool['description']}\")\n",
        "        if 'semantic_view' in tool:\n",
        "            print(f\"  Semantic View: {tool['semantic_view']}\")\n",
        "        if 'search_service' in tool:\n",
        "            print(f\"  Search Service: {tool['search_service']}\")\n",
        "else:\n",
        "    print(\"No agent data available to parse\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_semantic_view_queries(semantic_views: List[str]) -> List[str]:\n",
        "    \"\"\"\n",
        "    Generate SQL queries to extract YAML definitions from semantic views.\n",
        "    \n",
        "    Args:\n",
        "        semantic_views: List of semantic view names\n",
        "    \n",
        "    Returns:\n",
        "        List of SQL queries\n",
        "    \"\"\"\n",
        "    queries = []\n",
        "    \n",
        "    for semantic_view in semantic_views:\n",
        "        query = f\"SELECT SYSTEM$READ_YAML_FROM_SEMANTIC_VIEW('{semantic_view}') as yaml_content, '{semantic_view}' as semantic_view_name;\"\n",
        "        queries.append(query)\n",
        "    \n",
        "    return queries\n",
        "\n",
        "def extract_stage_info_from_semantic_model_file(semantic_model_file: str) -> Tuple[str, str, str]:\n",
        "    \"\"\"\n",
        "    Extract stage information from semantic model file path.\n",
        "    \n",
        "    Args:\n",
        "        semantic_model_file: Path like @DATABASE.SCHEMA.STAGE_NAME/file.yaml\n",
        "    \n",
        "    Returns:\n",
        "        Tuple of (database, schema, stage_name)\n",
        "    \"\"\"\n",
        "    if not semantic_model_file.startswith('@'):\n",
        "        return None, None, None\n",
        "    \n",
        "    stage_path = semantic_model_file[1:]  # Remove @ prefix\n",
        "    path_parts = stage_path.split('/')\n",
        "    \n",
        "    if len(path_parts) >= 1:\n",
        "        stage_identifier = path_parts[0]  # DATABASE.SCHEMA.STAGE_NAME\n",
        "        stage_parts = stage_identifier.split('.')\n",
        "        \n",
        "        if len(stage_parts) >= 3:\n",
        "            return stage_parts[0], stage_parts[1], stage_parts[2]\n",
        "    \n",
        "    return None, None, None\n",
        "\n",
        "def read_yaml_from_stage_connector(semantic_model_file: str) -> Optional[Dict]:\n",
        "    \"\"\"\n",
        "    Read YAML content from a stage using snowflake.connector (your approach).\n",
        "    \n",
        "    Args:\n",
        "        semantic_model_file: Path to semantic model file (e.g., @DATABASE.SCHEMA.STAGE/file.yaml)\n",
        "    \n",
        "    Returns:\n",
        "        Parsed YAML content as dictionary, or None if failed\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Extract stage information\n",
        "        database, schema, stage_name = extract_stage_info_from_semantic_model_file(semantic_model_file)\n",
        "        \n",
        "        if not all([database, schema, stage_name]):\n",
        "            print(f\"  ⚠️  Could not parse stage information from {semantic_model_file}\")\n",
        "            return None\n",
        "        \n",
        "        file_name = semantic_model_file.split('/')[-1]\n",
        "        \n",
        "        # Snowflake Connection Details\n",
        "        SNOWFLAKE_CONFIG = {\n",
        "            \"user\": os.getenv(\"SNOWFLAKE_USER\"),\n",
        "            \"password\": os.getenv(\"SNOWFLAKE_USER_PASSWORD\"),\n",
        "            \"account\": os.getenv(\"SNOWFLAKE_ACCOUNT\"),\n",
        "            \"warehouse\": os.getenv(\"SNOWFLAKE_WAREHOUSE\")\n",
        "        }\n",
        "        \n",
        "        # Setup Temporary Download Location\n",
        "        local_temp_dir = tempfile.mkdtemp()\n",
        "        local_file_path = os.path.join(local_temp_dir, file_name)\n",
        "        \n",
        "        print(f\"  📁 Temporary download location: {local_file_path}\")\n",
        "        \n",
        "        yaml_data = None\n",
        "        conn = None\n",
        "        \n",
        "        try:\n",
        "            # Connect to Snowflake\n",
        "            print(f\"  🔌 Connecting to Snowflake...\")\n",
        "            conn = snowflake.connector.connect(**SNOWFLAKE_CONFIG)\n",
        "            \n",
        "            # Format the GET command\n",
        "            stage_name_full = f\"@{database}.{schema}.{stage_name}\"\n",
        "            sql_get_command = f\"GET {stage_name_full}/{file_name} file://{os.path.normpath(local_temp_dir)}\"\n",
        "            \n",
        "            print(f\"  📥 Executing: {sql_get_command}\")\n",
        "            conn.cursor().execute(sql_get_command)\n",
        "            print(f\"  ✅ File downloaded successfully.\")\n",
        "            \n",
        "            # Read and Parse the local YAML file\n",
        "            print(f\"  🔍 Parsing YAML file...\")\n",
        "            with open(local_file_path, 'r') as f:\n",
        "                yaml_data = yaml.safe_load(f)\n",
        "            \n",
        "            print(f\"  ✅ YAML file parsed successfully!\")\n",
        "            return yaml_data\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"  ❌ Error during YAML processing: {e}\")\n",
        "            return None\n",
        "            \n",
        "        finally:\n",
        "            # Clean Up\n",
        "            if conn:\n",
        "                conn.close()\n",
        "                print(f\"  🔌 Snowflake connection closed.\")\n",
        "                \n",
        "            # Remove the temporary file and directory\n",
        "            try:\n",
        "                if os.path.exists(local_file_path):\n",
        "                    os.remove(local_file_path)\n",
        "                if os.path.exists(local_temp_dir):\n",
        "                    os.rmdir(local_temp_dir)\n",
        "                print(f\"  🧹 Temporary files cleaned up.\")\n",
        "            except Exception as e:\n",
        "                print(f\"  ⚠️  Error during cleanup: {e}\")\n",
        "                \n",
        "    except Exception as e:\n",
        "        print(f\"  ❌ Error reading YAML from stage: {e}\")\n",
        "        return None\n",
        "\n",
        "def extract_table_permissions_from_yaml(yaml_content: Dict) -> List[Tuple[str, str, str]]:\n",
        "    \"\"\"\n",
        "    Extract table permissions from parsed YAML content.\n",
        "    Enhanced to handle different YAML formats for semantic models.\n",
        "    \n",
        "    Args:\n",
        "        yaml_content: Parsed YAML content from semantic view or semantic model file\n",
        "    \n",
        "    Returns:\n",
        "        List of tuples containing (database, schema, table) for each table that needs SELECT permission\n",
        "    \"\"\"\n",
        "    table_permissions = []\n",
        "    \n",
        "    if not yaml_content:\n",
        "        return table_permissions\n",
        "    \n",
        "    # Method 1: Standard semantic view format\n",
        "    if \"tables\" in yaml_content:\n",
        "        for table in yaml_content[\"tables\"]:\n",
        "            if \"base_table\" in table:\n",
        "                base_table = table[\"base_table\"]\n",
        "                database = base_table.get(\"database\")\n",
        "                schema = base_table.get(\"schema\")\n",
        "                table_name = base_table.get(\"table\")\n",
        "                \n",
        "                if database and schema and table_name:\n",
        "                    table_permissions.append((database, schema, table_name))\n",
        "    \n",
        "    # Method 2: Alternative semantic model format\n",
        "    if \"semantic_model\" in yaml_content:\n",
        "        semantic_model = yaml_content[\"semantic_model\"]\n",
        "        \n",
        "        # Look for tables in different possible locations\n",
        "        if \"tables\" in semantic_model:\n",
        "            for table in semantic_model[\"tables\"]:\n",
        "                if isinstance(table, dict):\n",
        "                    # Extract table information\n",
        "                    database = table.get(\"database\") or table.get(\"db\")\n",
        "                    schema = table.get(\"schema\") or table.get(\"schema_name\")\n",
        "                    table_name = table.get(\"table\") or table.get(\"table_name\") or table.get(\"name\")\n",
        "                    \n",
        "                    if database and schema and table_name:\n",
        "                        table_permissions.append((database, schema, table_name))\n",
        "    \n",
        "    # Method 3: Look for table references in any nested structure\n",
        "    def find_table_references(obj, path=\"\"):\n",
        "        \"\"\"Recursively find table references in YAML structure\"\"\"\n",
        "        if isinstance(obj, dict):\n",
        "            for key, value in obj.items():\n",
        "                if key.lower() in ['table', 'base_table', 'source_table'] and isinstance(value, dict):\n",
        "                    database = value.get(\"database\") or value.get(\"db\")\n",
        "                    schema = value.get(\"schema\") or value.get(\"schema_name\")\n",
        "                    table_name = value.get(\"table\") or value.get(\"table_name\") or value.get(\"name\")\n",
        "                    \n",
        "                    if database and schema and table_name:\n",
        "                        table_permissions.append((database, schema, table_name))\n",
        "                elif isinstance(value, (dict, list)):\n",
        "                    find_table_references(value, f\"{path}.{key}\")\n",
        "        elif isinstance(obj, list):\n",
        "            for i, item in enumerate(obj):\n",
        "                if isinstance(item, (dict, list)):\n",
        "                    find_table_references(item, f\"{path}[{i}]\")\n",
        "    \n",
        "    # Run the recursive search\n",
        "    find_table_references(yaml_content)\n",
        "    \n",
        "    # Remove duplicates while preserving order\n",
        "    seen = set()\n",
        "    unique_permissions = []\n",
        "    for perm in table_permissions:\n",
        "        if perm not in seen:\n",
        "            seen.add(perm)\n",
        "            unique_permissions.append(perm)\n",
        "    \n",
        "    return unique_permissions\n",
        "\n",
        "print(\"Semantic view and semantic model file query functions defined successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate SQL queries for semantic views\n",
        "if example_agent_data:\n",
        "    parsed_tools = parse_agent_tools(example_agent_data)\n",
        "    semantic_view_queries = generate_semantic_view_queries(parsed_tools['semantic_views'])\n",
        "    \n",
        "    print(\"Generated SQL Queries for Semantic Views:\")\n",
        "    for i, query in enumerate(semantic_view_queries, 1):\n",
        "        print(f\"\\nQuery {i}:\")\n",
        "        print(query)\n",
        "else:\n",
        "    print(\"No agent data available to generate queries\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def setup_snowflake_connection() -> Optional[Session]:\n",
        "    \"\"\"\n",
        "    Set up Snowflake connection using environment variables.\n",
        "    \n",
        "    Returns:\n",
        "        Snowpark session or None if connection failed\n",
        "    \"\"\"\n",
        "    try:\n",
        "        connection_params = {\n",
        "            \"account\": os.getenv(\"SNOWFLAKE_ACCOUNT\"),\n",
        "            \"user\": os.getenv(\"SNOWFLAKE_USER\"),\n",
        "            \"password\": os.getenv(\"SNOWFLAKE_USER_PASSWORD\"),\n",
        "            \"role\": os.getenv(\"SNOWFLAKE_ROLE\"),\n",
        "            \"warehouse\": os.getenv(\"SNOWFLAKE_WAREHOUSE\")\n",
        "        }\n",
        "        \n",
        "        # Validate required parameters\n",
        "        required_params = [\"account\", \"user\", \"password\", \"role\", \"warehouse\"]\n",
        "        missing_params = [param for param in required_params if not connection_params[param]]\n",
        "        \n",
        "        if missing_params:\n",
        "            print(f\"Missing required environment variables: {missing_params}\")\n",
        "            return None\n",
        "        \n",
        "        session = Session.builder.configs(connection_params).create()\n",
        "        print(\"Successfully connected to Snowflake!\")\n",
        "        return session\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error connecting to Snowflake: {e}\")\n",
        "        return None\n",
        "\n",
        "def execute_semantic_view_queries(session: Session, semantic_views: List[str]) -> Dict[str, List[Tuple[str, str, str]]]:\n",
        "    \"\"\"\n",
        "    Execute semantic view queries and extract table permissions.\n",
        "    \n",
        "    Args:\n",
        "        session: Snowpark session\n",
        "        semantic_views: List of semantic view names\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary mapping semantic view names to their table permissions\n",
        "    \"\"\"\n",
        "    results = {}\n",
        "    \n",
        "    for semantic_view in semantic_views:\n",
        "        try:\n",
        "            print(f\"Processing semantic view: {semantic_view}\")\n",
        "            \n",
        "            # Execute the query\n",
        "            query = f\"SELECT SYSTEM$READ_YAML_FROM_SEMANTIC_VIEW('{semantic_view}') as yaml_content\"\n",
        "            result = session.sql(query).collect()\n",
        "            \n",
        "            if result and result[0]['YAML_CONTENT']:\n",
        "                # Parse YAML content\n",
        "                yaml_content = yaml.safe_load(result[0]['YAML_CONTENT'])\n",
        "                \n",
        "                # Extract table permissions\n",
        "                table_permissions = extract_table_permissions_from_yaml(yaml_content)\n",
        "                results[semantic_view] = table_permissions\n",
        "                \n",
        "                print(f\"  Found {len(table_permissions)} tables: {[f'{db}.{schema}.{table}' for db, schema, table in table_permissions]}\")\n",
        "                \n",
        "            else:\n",
        "                print(f\"  No YAML content found for {semantic_view}\")\n",
        "                results[semantic_view] = []\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"  Error processing {semantic_view}: {e}\")\n",
        "            results[semantic_view] = []\n",
        "    \n",
        "    return results\n",
        "\n",
        "def execute_semantic_model_file_queries(semantic_model_files: List[str]) -> Dict[str, List[Tuple[str, str, str]]]:\n",
        "    \"\"\"\n",
        "    Execute semantic model file queries and extract table permissions using snowflake.connector.\n",
        "    \n",
        "    Args:\n",
        "        semantic_model_files: List of semantic model file paths\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary mapping semantic model file names to their table permissions\n",
        "    \"\"\"\n",
        "    results = {}\n",
        "    \n",
        "    for semantic_model_file in semantic_model_files:\n",
        "        try:\n",
        "            print(f\"Processing semantic model file: {semantic_model_file}\")\n",
        "            \n",
        "            # Use the connector-based YAML reading method\n",
        "            yaml_content = read_yaml_from_stage_connector(semantic_model_file)\n",
        "            \n",
        "            if yaml_content:\n",
        "                print(f\"  ✅ Successfully read and parsed YAML file\")\n",
        "                \n",
        "                # Extract table permissions\n",
        "                table_permissions = extract_table_permissions_from_yaml(yaml_content)\n",
        "                results[semantic_model_file] = table_permissions\n",
        "                \n",
        "                print(f\"  ✅ Found {len(table_permissions)} tables: {[f'{db}.{schema}.{table}' for db, schema, table in table_permissions]}\")\n",
        "                \n",
        "                # Print YAML structure for debugging\n",
        "                print(f\"  🔍 YAML structure keys: {list(yaml_content.keys())}\")\n",
        "                \n",
        "            else:\n",
        "                print(f\"  ⚠️  Could not read YAML content from {semantic_model_file}\")\n",
        "                results[semantic_model_file] = []\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"  Error processing {semantic_model_file}: {e}\")\n",
        "            results[semantic_model_file] = []\n",
        "    \n",
        "    return results\n",
        "\n",
        "print(\"Snowflake connection and execution functions defined successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up Snowflake connection\n",
        "session = setup_snowflake_connection()\n",
        "\n",
        "# Execute semantic view queries if we have a connection and agent data\n",
        "if session and example_agent_data:\n",
        "    parsed_tools = parse_agent_tools(example_agent_data)\n",
        "    \n",
        "    # Process semantic views\n",
        "    if parsed_tools['semantic_views']:\n",
        "        print(\"\\nExecuting semantic view queries...\")\n",
        "        table_permissions_results = execute_semantic_view_queries(session, parsed_tools['semantic_views'])\n",
        "    else:\n",
        "        print(\"No semantic views found in agent specification\")\n",
        "        table_permissions_results = {}\n",
        "    \n",
        "    # Process semantic model files\n",
        "    if parsed_tools.get('semantic_model_files'):\n",
        "        print(\"\\nExecuting semantic model file queries...\")\n",
        "        semantic_model_results = execute_semantic_model_file_queries(parsed_tools['semantic_model_files'])\n",
        "        # Merge results\n",
        "        table_permissions_results.update(semantic_model_results)\n",
        "    else:\n",
        "        print(\"No semantic model files found in agent specification\")\n",
        "    \n",
        "    print(\"\\nTable Permissions Summary:\")\n",
        "    for resource_name, tables in table_permissions_results.items():\n",
        "        print(f\"\\n{resource_name}:\")\n",
        "        for db, schema, table in tables:\n",
        "            print(f\"  - {db}.{schema}.{table}\")\n",
        "else:\n",
        "    print(\"Skipping semantic view/model file execution - no connection or agent data available\")\n",
        "    table_permissions_results = {}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Updated function that includes agent location database/schema USAGE permissions, tool-specific warehouse permissions, and stage permissions\n",
        "def generate_comprehensive_permission_script_with_agent_location(\n",
        "    parsed_tools: Dict,\n",
        "    table_permissions_results: Dict[str, List[Tuple[str, str, str]]],\n",
        "    user_password: Optional[str] = None,\n",
        "    warehouse_name: str = \"COMPUTE_WH\"\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Generate comprehensive SQL permission script including agent location permissions, tool-specific warehouse permissions, and stage permissions.\n",
        "    \n",
        "    Args:\n",
        "        parsed_tools: Parsed tool information\n",
        "        table_permissions_results: Results from semantic view queries\n",
        "        user_password: Password for the created user\n",
        "        warehouse_name: Warehouse name for the user\n",
        "    \n",
        "    Returns:\n",
        "        Complete SQL script as string\n",
        "    \"\"\"\n",
        "    agent_name = parsed_tools[\"agent_name\"]\n",
        "    agent_database = parsed_tools[\"agent_database\"]\n",
        "    agent_schema = parsed_tools[\"agent_schema\"]\n",
        "    fully_qualified_agent = f\"{agent_database}.{agent_schema}.{agent_name}\"\n",
        "    \n",
        "    # Collect all unique table permissions\n",
        "    all_table_permissions = set()\n",
        "    for tables in table_permissions_results.values():\n",
        "        for db, schema, table in tables:\n",
        "            all_table_permissions.add(f\"{db}.{schema}.{table}\")\n",
        "    \n",
        "    # Generate database and schema USAGE grants specifically for each semantic view\n",
        "    semantic_view_db_grants = set()\n",
        "    semantic_view_schema_grants = set()\n",
        "    \n",
        "    for semantic_view in parsed_tools[\"semantic_views\"]:\n",
        "        parts = semantic_view.split('.')\n",
        "        if len(parts) == 3:\n",
        "            db_name, schema_name, view_name = parts\n",
        "            semantic_view_db_grants.add(db_name)\n",
        "            semantic_view_schema_grants.add(f\"{db_name}.{schema_name}\")\n",
        "    \n",
        "    # Generate database and schema USAGE grants for semantic model files (stages)\n",
        "    semantic_model_db_grants = set()\n",
        "    semantic_model_schema_grants = set()\n",
        "    stage_grants = set()\n",
        "    \n",
        "    for semantic_model_file in parsed_tools.get(\"semantic_model_files\", []):\n",
        "        database, schema, stage_name = extract_stage_info_from_semantic_model_file(semantic_model_file)\n",
        "        if all([database, schema, stage_name]):\n",
        "            semantic_model_db_grants.add(database)\n",
        "            semantic_model_schema_grants.add(f\"{database}.{schema}\")\n",
        "            stage_grants.add(f\"{database}.{schema}.{stage_name}\")\n",
        "    \n",
        "    # Generate database and schema USAGE grants for search services\n",
        "    search_db_grants = set()\n",
        "    search_schema_grants = set()\n",
        "    \n",
        "    for search_service in parsed_tools[\"search_services\"]:\n",
        "        parts = search_service.split('.')\n",
        "        if len(parts) == 3:\n",
        "            db_name, schema_name, service_name = parts\n",
        "            search_db_grants.add(db_name)\n",
        "            search_schema_grants.add(f\"{db_name}.{schema_name}\")\n",
        "    \n",
        "    # Generate database and schema USAGE grants for procedures\n",
        "    procedure_db_grants = set()\n",
        "    procedure_schema_grants = set()\n",
        "    \n",
        "    for procedure in parsed_tools.get(\"procedures\", []):\n",
        "        # Extract database and schema from procedure identifier\n",
        "        # Format: DATABASE.SCHEMA.PROCEDURE_NAME(PARAM_TYPES)\n",
        "        procedure_parts = procedure.split('(')[0]  # Remove parameter types\n",
        "        parts = procedure_parts.split('.')\n",
        "        if len(parts) >= 3:\n",
        "            db_name, schema_name, proc_name = parts\n",
        "            procedure_db_grants.add(db_name)\n",
        "            procedure_schema_grants.add(f\"{db_name}.{schema_name}\")\n",
        "    \n",
        "    # Add agent's own database and schema USAGE grants\n",
        "    agent_db_grants = {agent_database}\n",
        "    agent_schema_grants = {f\"{agent_database}.{agent_schema}\"}\n",
        "    \n",
        "    # CRITICAL FIX: Add database and schema grants for tables discovered in semantic view YAML\n",
        "    # that are not already covered by the agent tool specifications\n",
        "    table_db_grants = set()\n",
        "    table_schema_grants = set()\n",
        "    \n",
        "    for tables in table_permissions_results.values():\n",
        "        for db, schema, table in tables:\n",
        "            table_db_grants.add(db)\n",
        "            table_schema_grants.add(f\"{db}.{schema}\")\n",
        "    \n",
        "    # Combine all database and schema grants\n",
        "    all_db_grants = semantic_view_db_grants.union(search_db_grants).union(agent_db_grants).union(table_db_grants).union(procedure_db_grants).union(semantic_model_db_grants)\n",
        "    all_schema_grants = semantic_view_schema_grants.union(search_schema_grants).union(agent_schema_grants).union(table_schema_grants).union(procedure_schema_grants).union(semantic_model_schema_grants)\n",
        "    \n",
        "    # Generate permission grants\n",
        "    db_grants = \"\\n\".join([f\"GRANT USAGE ON DATABASE {db} TO ROLE IDENTIFIER($AGENT_ROLE_NAME);\" \n",
        "                          for db in sorted(all_db_grants)])\n",
        "    \n",
        "    schema_grants = \"\\n\".join([f\"GRANT USAGE ON SCHEMA {schema} TO ROLE IDENTIFIER($AGENT_ROLE_NAME);\" \n",
        "                              for schema in sorted(all_schema_grants)])\n",
        "    \n",
        "    view_grants = \"\\n\".join([f\"GRANT SELECT ON VIEW {view} TO ROLE IDENTIFIER($AGENT_ROLE_NAME);\" \n",
        "                            for view in sorted(parsed_tools[\"semantic_views\"])])\n",
        "    \n",
        "    table_grants = \"\\n\".join([f\"GRANT SELECT ON TABLE {table} TO ROLE IDENTIFIER($AGENT_ROLE_NAME);\" \n",
        "                            for table in sorted(all_table_permissions)])\n",
        "    \n",
        "    search_grants = \"\\n\".join([f\"GRANT USAGE ON CORTEX SEARCH SERVICE {service} TO ROLE IDENTIFIER($AGENT_ROLE_NAME);\" \n",
        "                              for service in sorted(parsed_tools[\"search_services\"])])\n",
        "    \n",
        "    # Generate procedure grants\n",
        "    procedure_grants = \"\\n\".join([f\"GRANT USAGE ON PROCEDURE {procedure} TO ROLE IDENTIFIER($AGENT_ROLE_NAME);\" \n",
        "                                for procedure in sorted(parsed_tools.get(\"procedures\", []))])\n",
        "    \n",
        "    # Generate stage grants for semantic model files\n",
        "    stage_grants_sql = \"\"\n",
        "    if stage_grants:\n",
        "        stage_grants_sql = \"\\n\".join([f\"GRANT READ ON STAGE {stage} TO ROLE IDENTIFIER($AGENT_ROLE_NAME);\" \n",
        "                                     for stage in sorted(stage_grants)])\n",
        "        stage_grants_sql = f\"\\n{stage_grants_sql}\"\n",
        "    \n",
        "    # Generate tool-specific warehouse grants\n",
        "    tool_warehouse_grants = \"\"\n",
        "    if parsed_tools.get(\"tool_warehouses\"):\n",
        "        tool_warehouse_grants = \"\\n\".join([\n",
        "            f\"GRANT USAGE ON WAREHOUSE IDENTIFIER('{warehouse}') TO ROLE IDENTIFIER($AGENT_ROLE_NAME); -- Required for tool: {tool_name}\"\n",
        "            for tool_name, warehouse in parsed_tools[\"tool_warehouses\"].items()\n",
        "        ])\n",
        "        if tool_warehouse_grants:\n",
        "            tool_warehouse_grants = f\"\\n-- Tool-specific warehouse permissions\\n{tool_warehouse_grants}\"\n",
        "    \n",
        "    # Assemble the complete script\n",
        "    script = f\"\"\"-- =========================================================================================\n",
        "-- AUTO-GENERATED LEAST-PRIVILEGE SCRIPT FOR AGENT: {fully_qualified_agent}\n",
        "-- Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "-- Generated by: Automated Agent Permissions Notebook\n",
        "-- =========================================================================================\n",
        "\n",
        "-- IMPORTANT: Review and adjust the placeholder variables below for your environment.\n",
        "SET AGENT_ROLE_NAME = '{agent_name}_USER_ROLE';\n",
        "SET WAREHOUSE_NAME = '{warehouse_name}';\n",
        "\n",
        "-- Create a dedicated role for the agent's permissions.\n",
        "USE ROLE SECURITYADMIN; -- Or your own privileged role to assign permissions\n",
        "CREATE ROLE IF NOT EXISTS IDENTIFIER($AGENT_ROLE_NAME);\n",
        "GRANT ROLE IDENTIFIER($AGENT_ROLE_NAME) TO ROLE SYSADMIN; -- Optional: Allows SYSADMIN to manage the role.\n",
        "\n",
        "-- Grant core permission to use the agent object itself.\n",
        "GRANT USAGE ON AGENT {fully_qualified_agent} TO ROLE IDENTIFIER($AGENT_ROLE_NAME);\n",
        "\n",
        "-- Grant permissions on the underlying database objects required by the agent's tools.\n",
        "-- NOTE: These permissions are derived from the agent's tool specification and semantic view YAML definitions.\n",
        "\n",
        "-- Database and Schema USAGE grants (including agent location, tool-specific locations, tables from semantic views, procedures, and stages)\n",
        "{db_grants}\n",
        "{schema_grants}\n",
        "\n",
        "-- Permissions for 'cortex_analyst_text_to_sql' tools\n",
        "-- Semantic view permissions\n",
        "{view_grants}\n",
        "\n",
        "-- Base table permissions (from semantic view YAML)\n",
        "{table_grants}\n",
        "\n",
        "-- Permissions for 'cortex_search' tools\n",
        "{search_grants}\n",
        "\n",
        "-- Permissions for 'generic' tools (procedures)\n",
        "{procedure_grants}\n",
        "\n",
        "-- Stage permissions for semantic model files\n",
        "{stage_grants_sql}\n",
        "\n",
        "{tool_warehouse_grants}\n",
        "\n",
        "-- Grant warehouse usage to the role for the user's session.\n",
        "GRANT USAGE ON WAREHOUSE IDENTIFIER($WAREHOUSE_NAME) TO ROLE IDENTIFIER($AGENT_ROLE_NAME);\n",
        "\n",
        "-- =========================================================================================\n",
        "SELECT 'Setup complete for role ' || $AGENT_ROLE_NAME AS \"Status\";\n",
        "-- =========================================================================================\n",
        "\"\"\"\n",
        "    \n",
        "    return script\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the updated function with agent location permissions and tool-specific warehouse permissions\n",
        "if example_agent_data:\n",
        "    parsed_tools = parse_agent_tools(example_agent_data)\n",
        "    \n",
        "    # Use actual table permissions if available, otherwise use empty results\n",
        "    if 'table_permissions_results' in locals():\n",
        "        final_table_permissions = table_permissions_results\n",
        "    else:\n",
        "        final_table_permissions = {}\n",
        "    \n",
        "    # Generate script with the updated function that includes agent location and tool-specific warehouses\n",
        "    updated_permission_script = generate_comprehensive_permission_script_with_agent_location(\n",
        "        parsed_tools=parsed_tools,\n",
        "        table_permissions_results=final_table_permissions,\n",
        "        warehouse_name=\"COMPUTE_WH\"\n",
        "    )\n",
        "    \n",
        "    #print(\"Updated Permission Script (with Agent Location and Tool-Specific Warehouses):\")\n",
        "    print(\"--\",\"=\" * 80)\n",
        "    print(updated_permission_script)\n",
        "    \n",
        "        \n",
        "else:\n",
        "    print(\"No agent data available to test updated function\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "getting_started_llmops",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
